import{_ as s,o as t,c as n,ag as o}from"./chunks/framework.CbQjVMS6.js";const c=JSON.parse('{"title":"手撕 GPT：从一行文本到下一词预测的完整链路","description":"","frontmatter":{},"headers":[],"relativePath":"ai/build-gpt-from-scratch.md","filePath":"ai/build-gpt-from-scratch.md"}'),i={name:"ai/build-gpt-from-scratch.md"};function e(p,a,l,r,h,d){return t(),n("div",null,[...a[0]||(a[0]=[o(`<h1 id="手撕-gpt-从一行文本到下一词预测的完整链路" tabindex="-1">手撕 GPT：从一行文本到下一词预测的完整链路 <a class="header-anchor" href="#手撕-gpt-从一行文本到下一词预测的完整链路" aria-label="Permalink to &quot;手撕 GPT：从一行文本到下一词预测的完整链路&quot;">​</a></h1><blockquote><p>这不是一篇&quot;大模型科普&quot;，而是我在 7 天内从零构建 GPT 的过程中，真正理解的那些东西。</p></blockquote><h2 id="为什么要从零构建" tabindex="-1">为什么要从零构建？ <a class="header-anchor" href="#为什么要从零构建" aria-label="Permalink to &quot;为什么要从零构建？&quot;">​</a></h2><p>市面上不缺大模型教程，缺的是 <strong>&quot;拆开黑箱之后的确定感&quot;</strong>。</p><p>我给自己定了一个目标：用一周时间，跟着 Sebastian Raschka 的《Build a Large Language Model (From Scratch)》，亲手用 PyTorch 实现一个类 GPT 模型——从分词器到注意力机制，从预训练到微调，每一行核心代码手写，不调现成 API。</p><p>学习方法很简单：<strong>每遇到一个不理解的概念，就把它变成一个问题，逼自己用大白话回答出来。</strong> 如果回答不了，说明没真懂。这篇博客就是这些问答的提炼。</p><hr><h2 id="一句话的完整旅程" tabindex="-1">一句话的完整旅程 <a class="header-anchor" href="#一句话的完整旅程" aria-label="Permalink to &quot;一句话的完整旅程&quot;">​</a></h2><p>为了把整条链路讲透，我们追踪一个具体例子：模型如何学会&quot;看到 <code>你好世界</code> 就预测出 <code>啊</code>&quot;。</p><p>整条数据流水线长这样：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>文本 → 分词器(BPE) → Token ID → 嵌入层(向量化) → +位置编码 → Transformer(注意力) → 预测下一个词</span></span>
<span class="line"><span>       固定映射       编号      可训练向量        知道顺序      学词间关系</span></span></code></pre></div><p>下面逐段拆解。</p><hr><h2 id="第一站-文本变数字-—-分词与编码" tabindex="-1">第一站：文本变数字 — 分词与编码 <a class="header-anchor" href="#第一站-文本变数字-—-分词与编码" aria-label="Permalink to &quot;第一站：文本变数字 — 分词与编码&quot;">​</a></h2><p>模型不认字，只认数字。第一步是把文本拆成 <strong>Token</strong>，再映射为 ID。</p><p>GPT 系列使用 <strong>BPE（字节对编码）</strong> 算法。它的精妙之处在于：常见词保持完整，生僻词拆成子词，<strong>任何文本都能编码，不存在&quot;未知词&quot;</strong>。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tiktoken</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tiktoken.get_encoding(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer.encode(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;hello&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)          </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># → [31373]         常见词 1 个 token</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer.encode(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;tokenization&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)   </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># → [30001, 1634]   生词拆成 2 个</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer.encode(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;你好&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)           </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># → [19526, 254, 25001, 121]  中文 4 个 token</span></span></code></pre></div><p><strong>为什么不按空格拆词？</strong> 词表会爆炸，且无法处理新词。<strong>为什么不按字符拆？</strong> 序列太长，计算效率极低。BPE 在两者之间找到了平衡。</p><p>不同模型的词表规模差异巨大：</p><table tabindex="0"><thead><tr><th>模型</th><th>词表大小</th><th>编码方案</th></tr></thead><tbody><tr><td>GPT-2</td><td>50,257</td><td><code>gpt2</code></td></tr><tr><td>GPT-3.5/4</td><td>100,256</td><td><code>cl100k_base</code></td></tr><tr><td>GPT-4o</td><td>200,019</td><td><code>o200k_base</code></td></tr></tbody></table><p>词表越大，同样的文本拆出的 token 越少（尤其是多语言场景），推理效率越高——但嵌入矩阵也更大。这是工程上的取舍。</p><hr><h2 id="第二站-数字变向量-—-嵌入层的本质" tabindex="-1">第二站：数字变向量 — 嵌入层的本质 <a class="header-anchor" href="#第二站-数字变向量-—-嵌入层的本质" aria-label="Permalink to &quot;第二站：数字变向量 — 嵌入层的本质&quot;">​</a></h2><p>Token ID 只是编号，<code>15496</code> 并不比 <code>995</code> &quot;更有意义&quot;。我们需要把它变成一个 <strong>高维向量</strong>，让数字承载语义。</p><p>嵌入层的本质就是一张 <strong>查找表（Lookup Table）</strong>：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embedding </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.Embedding(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50257</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">256</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 权重矩阵形状：[50257, 256]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 每一行是一个 token 的 256 维向量</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">embedding(torch.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3797</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]))   </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输入 token ID → 输出第 3797 行向量</span></span></code></pre></div><p>没有任何复杂计算，就是&quot;查行&quot;。但关键在于：<strong>这张表里的每个数字都是可训练参数。</strong></p><p>初始时向量是随机的。训练之后：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&quot;cat&quot;  → [0.2, -0.5, 0.8, ...]   ← 语义相近的词</span></span>
<span class="line"><span>&quot;dog&quot;  → [0.3, -0.4, 0.7, ...]   ← 向量距离很近</span></span>
<span class="line"><span>&quot;the&quot;  → [-0.1, 0.9, -0.3, ...]  ← 向量距离很远</span></span></code></pre></div><p>除了词嵌入，还需要 <strong>位置嵌入（Positional Embedding）</strong>。因为注意力机制本身不感知顺序——&quot;猫追狗&quot;和&quot;狗追猫&quot;如果没有位置信息，对模型来说是一样的。位置嵌入让模型知道每个词在序列中的位置。</p><hr><h2 id="第三站-学会-看关系-—-注意力机制" tabindex="-1">第三站：学会&quot;看关系&quot; — 注意力机制 <a class="header-anchor" href="#第三站-学会-看关系-—-注意力机制" aria-label="Permalink to &quot;第三站：学会&quot;看关系&quot; — 注意力机制&quot;">​</a></h2><p>单个词的含义靠嵌入解决了，但词与词之间的关系呢？&quot;苹果&quot;在&quot;吃苹果&quot;和&quot;苹果公司&quot;中含义完全不同。这就是注意力机制要解决的问题。</p><h3 id="核心思想-每个词都在-提问" tabindex="-1">核心思想：每个词都在&quot;提问&quot; <a class="header-anchor" href="#核心思想-每个词都在-提问" aria-label="Permalink to &quot;核心思想：每个词都在&quot;提问&quot;&quot;">​</a></h3><p>每个词元被线性变换为三个向量：</p><ul><li><strong>Query（查询）</strong>：我在找什么样的信息？</li><li><strong>Key（键）</strong>：我能提供什么样的信息？</li><li><strong>Value（值）</strong>：我的实际内容是什么？</li></ul><p>通过计算 Query 和 Key 的 <strong>点积</strong>，模型得到一个相关性分数。点积值越高，两个词在特征空间中对齐程度越高——注意力就越多。</p><h3 id="为什么要-缩放" tabindex="-1">为什么要&quot;缩放&quot;？ <a class="header-anchor" href="#为什么要-缩放" aria-label="Permalink to &quot;为什么要&quot;缩放&quot;？&quot;">​</a></h3><p>当嵌入维度 d_k 很大时，点积的量级会膨胀，导致 Softmax 输出变得极端（几乎所有权重集中在一个词上）。从梯度角度看，Softmax 进入饱和区，梯度趋近于零，训练无法收敛。</p><p><strong>除以 √d_k</strong> 把数值拉回梯度敏感区，这就是 Scaled Dot-Product Attention 的由来。</p><h3 id="因果掩码-禁止-偷看未来" tabindex="-1">因果掩码：禁止&quot;偷看未来&quot; <a class="header-anchor" href="#因果掩码-禁止-偷看未来" aria-label="Permalink to &quot;因果掩码：禁止&quot;偷看未来&quot;&quot;">​</a></h3><p>GPT 是自回归模型，生成第 3 个词时只能看到前 2 个词。实现方式是在 Softmax 之前，把&quot;未来位置&quot;的注意力分数设为 <strong>-∞</strong>，经过 Softmax 后权重精确变为 0：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>         位置1  位置2  位置3  位置4</span></span>
<span class="line"><span>位置1  [ 0.8   -∞     -∞     -∞  ]</span></span>
<span class="line"><span>位置2  [ 0.3   0.5    -∞     -∞  ]</span></span>
<span class="line"><span>位置3  [ 0.1   0.4    0.3    -∞  ]</span></span>
<span class="line"><span>位置4  [ 0.2   0.1    0.3    0.4 ]</span></span></code></pre></div><h3 id="多头注意力-多个视角并行观察" tabindex="-1">多头注意力：多个视角并行观察 <a class="header-anchor" href="#多头注意力-多个视角并行观察" aria-label="Permalink to &quot;多头注意力：多个视角并行观察&quot;">​</a></h3><p>单头注意力只能从一个角度看关系。多头注意力把向量投影到多个子空间，每个&quot;头&quot;独立捕获不同类型的特征——有的关注语法关系，有的关注语义关联，有的关注距离模式。</p><p>所有头的输出拼接后通过线性层融合，将碎片化的信息重组为统一表示。</p><hr><h2 id="第四站-搭积木-—-transformer-block" tabindex="-1">第四站：搭积木 — Transformer Block <a class="header-anchor" href="#第四站-搭积木-—-transformer-block" aria-label="Permalink to &quot;第四站：搭积木 — Transformer Block&quot;">​</a></h2><p>有了注意力机制，接下来把它组装成完整的 Transformer 块。GPT-2 (124M) 堆叠了 12 个这样的块，GPT-3 堆叠了 96 个。</p><p>每个块包含四个关键组件：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>输入 ──→ 层归一化 → 多头注意力 ──→ (+残差) → 层归一化 → 前馈网络 ──→ (+残差) → 输出</span></span></code></pre></div><p><strong>每个组件都有清晰的设计理由：</strong></p><p><strong>层归一化（Layer Normalization）</strong> — 将激活值标准化为均值 0、方差 1。没有它，激活值会随层数加深而爆炸或消失，模型根本训练不动。有了它，可以用更大的学习率，训练更快收敛。</p><p><strong>GELU 激活函数</strong> — 传统 ReLU 在输入为负时输出恒为 0（&quot;死区&quot;），梯度完全消失。GELU 提供平滑的非零输出，即使负值也保留梯度信息，在深层网络中表现更优。</p><p><strong>残差连接（Residual Connection）</strong> — <code>x + Sublayer(x)</code> 结构。反向传播时梯度可以直接&quot;跳过&quot;复杂层，无损传回浅层。这是 96 层深度的 GPT-3 能够成功训练的关键——没有残差连接，深度网络的梯度会逐层衰减到几乎为零。</p><p><strong>前馈网络（FFN）</strong> — 两个线性层夹一个 GELU。如果说注意力机制负责&quot;词与词之间的关系&quot;，前馈网络就负责&quot;每个词自身的特征变换&quot;，两者协同工作。</p><hr><h2 id="第五站-训练-—-自监督的下一词预测" tabindex="-1">第五站：训练 — 自监督的下一词预测 <a class="header-anchor" href="#第五站-训练-—-自监督的下一词预测" aria-label="Permalink to &quot;第五站：训练 — 自监督的下一词预测&quot;">​</a></h2><p>模型搭好了，怎么训练？GPT 的训练目标极其简洁：<strong>给定前面的词，预测下一个词。</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>训练文本：&quot;你好世界啊&quot;，max_length=4</span></span>
<span class="line"><span></span></span>
<span class="line"><span>input_ids（输入）:   [你, 好, 世, 界]    → 喂给模型看的</span></span>
<span class="line"><span>target_ids（目标）:  [好, 世, 界, 啊]    → 模型应该预测出的正确答案</span></span></code></pre></div><p>target 就是 input <strong>往右移了一位</strong>。模型需要同时学会：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>看到 &quot;你&quot;           → 预测 &quot;好&quot;</span></span>
<span class="line"><span>看到 &quot;你, 好&quot;       → 预测 &quot;世&quot;</span></span>
<span class="line"><span>看到 &quot;你, 好, 世&quot;   → 预测 &quot;界&quot;</span></span>
<span class="line"><span>看到 &quot;你, 好, 世, 界&quot; → 预测 &quot;啊&quot;</span></span></code></pre></div><p><strong>这就是自监督学习的美妙之处：标签不需要人工标注，文本自身的下一个词就是答案。</strong> GPT-3 的训练语料约 3000 亿词元，包括 CommonCrawl、WebText、Wikipedia 等，全部都是这样自动构造的训练对。</p><p>模型的预测结果通过 <strong>交叉熵损失（Cross-Entropy Loss）</strong> 与真实目标对比。如果目标词是&quot;界&quot;但模型给了很低的概率，损失就会很大，反向传播就会强力修正。</p><p>训练中用 <strong>滑动窗口</strong> 遍历全部文本。窗口大小决定上下文长度，步长决定窗口移动距离。步长为 1 时训练样本高度重叠，模型学到的转换关系最细致。</p><hr><h2 id="第六站-生成-—-从概率到文字" tabindex="-1">第六站：生成 — 从概率到文字 <a class="header-anchor" href="#第六站-生成-—-从概率到文字" aria-label="Permalink to &quot;第六站：生成 — 从概率到文字&quot;">​</a></h2><p>训练完成后，模型能为词表中每个词输出一个概率。但怎么选词？</p><p><strong>贪婪搜索</strong>（永远选概率最高的词）看似最优，实际会导致文本枯燥重复，甚至陷入死循环。工程上需要更聪明的策略：</p><p><strong>温度缩放（Temperature Scaling）</strong> — 控制概率分布的&quot;尖锐度&quot;：</p><ul><li><strong>T &lt; 1（低温度）</strong>：分布更尖锐，高概率词优势放大 → 输出保守、连贯，适合要求准确性的场景</li><li><strong>T &gt; 1（高温度）</strong>：分布被拉平，各词概率差距缩小 → 输出更有创意和多样性，但可能降低逻辑性</li></ul><p><strong>Top-k 采样</strong> — 只从概率最高的 k 个词中随机采样，砍掉长尾低概率词。这些低概率词是&quot;逻辑断裂&quot;的元凶——它们单独出现概率极低，一旦被选中就会让句子偏离轨道。Top-k 在随机性和稳定性之间找到了平衡。</p><hr><h2 id="认知升级-从零构建之后的三个顿悟" tabindex="-1">认知升级：从零构建之后的三个顿悟 <a class="header-anchor" href="#认知升级-从零构建之后的三个顿悟" aria-label="Permalink to &quot;认知升级：从零构建之后的三个顿悟&quot;">​</a></h2><h3 id="_1-大模型的-理解-本质上是统计" tabindex="-1">1. 大模型的&quot;理解&quot;本质上是统计 <a class="header-anchor" href="#_1-大模型的-理解-本质上是统计" aria-label="Permalink to &quot;1. 大模型的&quot;理解&quot;本质上是统计&quot;">​</a></h3><p>GPT 不是&quot;理解&quot;了语言，而是在海量文本的统计模式中学到了极其精密的条件概率分布。但当这个分布精密到一定程度，涌现出的行为看起来就像&quot;理解&quot;。这不是降低它的价值，而是让我对它的能力边界有了更清晰的认知。</p><h3 id="_2-工程设计处处是取舍" tabindex="-1">2. 工程设计处处是取舍 <a class="header-anchor" href="#_2-工程设计处处是取舍" aria-label="Permalink to &quot;2. 工程设计处处是取舍&quot;">​</a></h3><ul><li>词表大小：更大的词表让多语言编码更高效，但嵌入矩阵占用更多显存</li><li>缩放注意力：不缩放数学上也能算，但梯度会消失，训练不动</li><li>残差连接：看起来只是&quot;加了一下&quot;，却是深度网络能训练的根本保障</li><li>温度参数：没有&quot;最优值&quot;，只有&quot;适合当前任务的值&quot;</li></ul><p><strong>每一个看似简单的设计选择，背后都是对&quot;在当前约束下什么最有效&quot;的回答。</strong></p><h3 id="_3-最好的学习方式是-自问自答" tabindex="-1">3. 最好的学习方式是&quot;自问自答&quot; <a class="header-anchor" href="#_3-最好的学习方式是-自问自答" aria-label="Permalink to &quot;3. 最好的学习方式是&quot;自问自答&quot;&quot;">​</a></h3><p>整个学习过程中，我发现最有效的方法不是看完就过，而是每遇到一个概念就问自己：&quot;如果面试官问我这个，我能用 30 秒讲清楚吗？&quot;讲不清楚就说明理解有黑洞。把这些问题记录下来，逼自己用最通俗的语言回答——这篇博客里的每一段，都是这样打磨出来的。</p><hr><h2 id="写在最后" tabindex="-1">写在最后 <a class="header-anchor" href="#写在最后" aria-label="Permalink to &quot;写在最后&quot;">​</a></h2><p>从零构建大模型这件事，最大的收获不是&quot;会写一个 GPT&quot;，而是 <strong>拆掉了对黑箱的恐惧</strong>。</p><p>当你亲手写过嵌入层的查表逻辑、手算过注意力矩阵的掩码、看过 Loss 从天文数字降到个位数——你对这个领域的理解就从&quot;知道它很厉害&quot;变成了&quot;知道它为什么厉害、以及厉害在哪里&quot;。</p><p>如果你也在学习 AI，推荐 Sebastian Raschka 的 <em>Build a Large Language Model (From Scratch)</em>。不用 GPU 也能跑前四章的代码，CPU 足够。关键是：<strong>核心代码一定要手写，不要复制粘贴。</strong> 手写一遍 Attention 类的理解深度，和看十遍教程完全不同。</p>`,86)])])}const g=s(i,[["render",e]]);export{c as __pageData,g as default};
